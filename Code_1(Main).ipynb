{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2846998f",
   "metadata": {},
   "source": [
    "1. Random Forest classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bf92f0f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.05838202\n",
      "Iteration 2, loss = 0.03388066\n",
      "Iteration 3, loss = 0.02810062\n",
      "Iteration 4, loss = 0.03048016\n",
      "Iteration 5, loss = 0.02682925\n",
      "Iteration 6, loss = 0.02729952\n",
      "Iteration 7, loss = 0.02411756\n",
      "Iteration 8, loss = 0.02716487\n",
      "Iteration 9, loss = 0.02309559\n",
      "Iteration 10, loss = 0.02251507\n",
      "Iteration 11, loss = 0.02223553\n",
      "Iteration 12, loss = 0.02275422\n",
      "Iteration 13, loss = 0.02096732\n",
      "Iteration 14, loss = 0.01999285\n",
      "Iteration 15, loss = 0.02105753\n",
      "Iteration 16, loss = 0.02177573\n",
      "Iteration 17, loss = 0.01987187\n",
      "Iteration 18, loss = 0.01911567\n",
      "Iteration 19, loss = 0.02065341\n",
      "Iteration 20, loss = 0.01965351\n",
      "Iteration 21, loss = 0.01917630\n",
      "Iteration 22, loss = 0.01934509\n",
      "Iteration 23, loss = 0.01924165\n",
      "Iteration 24, loss = 0.01923527\n",
      "Iteration 25, loss = 0.01992322\n",
      "Iteration 26, loss = 0.01954303\n",
      "Iteration 27, loss = 0.01881877\n",
      "Iteration 28, loss = 0.01886583\n",
      "Iteration 29, loss = 0.01948347\n",
      "Iteration 30, loss = 0.01846505\n",
      "Iteration 31, loss = 0.01806314\n",
      "Iteration 32, loss = 0.01676949\n",
      "Iteration 33, loss = 0.01836120\n",
      "Iteration 34, loss = 0.01841488\n",
      "Iteration 35, loss = 0.01731922\n",
      "Iteration 36, loss = 0.01769060\n",
      "Iteration 37, loss = 0.01833698\n",
      "Iteration 38, loss = 0.01771931\n",
      "Iteration 39, loss = 0.01712982\n",
      "Iteration 40, loss = 0.01759364\n",
      "Iteration 41, loss = 0.01692882\n",
      "Iteration 42, loss = 0.01781285\n",
      "Iteration 43, loss = 0.01650783\n",
      "Iteration 44, loss = 0.01706146\n",
      "Iteration 45, loss = 0.01658269\n",
      "Iteration 46, loss = 0.01688384\n",
      "Iteration 47, loss = 0.01688886\n",
      "Iteration 48, loss = 0.01650350\n",
      "Iteration 49, loss = 0.01748605\n",
      "Iteration 50, loss = 0.01617083\n",
      "Iteration 51, loss = 0.01639577\n",
      "Iteration 52, loss = 0.01687977\n",
      "Iteration 53, loss = 0.01567994\n",
      "Iteration 54, loss = 0.01669733\n",
      "Iteration 55, loss = 0.01701371\n",
      "Iteration 56, loss = 0.01624219\n",
      "Iteration 57, loss = 0.01625426\n",
      "Iteration 58, loss = 0.01614088\n",
      "Iteration 59, loss = 0.01616373\n",
      "Iteration 60, loss = 0.01703415\n",
      "Iteration 61, loss = 0.01619793\n",
      "Iteration 62, loss = 0.01679048\n",
      "Iteration 63, loss = 0.01560885\n",
      "Iteration 64, loss = 0.01628632\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05972752\n",
      "Iteration 2, loss = 0.03491899\n",
      "Iteration 3, loss = 0.03327066\n",
      "Iteration 4, loss = 0.02911952\n",
      "Iteration 5, loss = 0.02532502\n",
      "Iteration 6, loss = 0.02700477\n",
      "Iteration 7, loss = 0.02324076\n",
      "Iteration 8, loss = 0.02503371\n",
      "Iteration 9, loss = 0.02358689\n",
      "Iteration 10, loss = 0.02300386\n",
      "Iteration 11, loss = 0.02176662\n",
      "Iteration 12, loss = 0.02111966\n",
      "Iteration 13, loss = 0.02153561\n",
      "Iteration 14, loss = 0.01989756\n",
      "Iteration 15, loss = 0.02108086\n",
      "Iteration 16, loss = 0.02146230\n",
      "Iteration 17, loss = 0.02114024\n",
      "Iteration 18, loss = 0.01988557\n",
      "Iteration 19, loss = 0.02053457\n",
      "Iteration 20, loss = 0.01969763\n",
      "Iteration 21, loss = 0.02088647\n",
      "Iteration 22, loss = 0.01935521\n",
      "Iteration 23, loss = 0.02122188\n",
      "Iteration 24, loss = 0.01872627\n",
      "Iteration 25, loss = 0.01893867\n",
      "Iteration 26, loss = 0.01843635\n",
      "Iteration 27, loss = 0.01774503\n",
      "Iteration 28, loss = 0.01990272\n",
      "Iteration 29, loss = 0.01830021\n",
      "Iteration 30, loss = 0.01847853\n",
      "Iteration 31, loss = 0.01782678\n",
      "Iteration 32, loss = 0.01708463\n",
      "Iteration 33, loss = 0.01851754\n",
      "Iteration 34, loss = 0.01749886\n",
      "Iteration 35, loss = 0.01807792\n",
      "Iteration 36, loss = 0.01695011\n",
      "Iteration 37, loss = 0.01797944\n",
      "Iteration 38, loss = 0.01717983\n",
      "Iteration 39, loss = 0.01765793\n",
      "Iteration 40, loss = 0.01837218\n",
      "Iteration 41, loss = 0.01638170\n",
      "Iteration 42, loss = 0.01671712\n",
      "Iteration 43, loss = 0.01622136\n",
      "Iteration 44, loss = 0.01727567\n",
      "Iteration 45, loss = 0.01675653\n",
      "Iteration 46, loss = 0.01727652\n",
      "Iteration 47, loss = 0.01673948\n",
      "Iteration 48, loss = 0.01750434\n",
      "Iteration 49, loss = 0.01594648\n",
      "Iteration 50, loss = 0.01701663\n",
      "Iteration 51, loss = 0.01602010\n",
      "Iteration 52, loss = 0.01662479\n",
      "Iteration 53, loss = 0.01672377\n",
      "Iteration 54, loss = 0.01658985\n",
      "Iteration 55, loss = 0.01693535\n",
      "Iteration 56, loss = 0.01676359\n",
      "Iteration 57, loss = 0.01616107\n",
      "Iteration 58, loss = 0.01746575\n",
      "Iteration 59, loss = 0.01576051\n",
      "Iteration 60, loss = 0.01616950\n",
      "Iteration 61, loss = 0.01684090\n",
      "Iteration 62, loss = 0.01532975\n",
      "Iteration 63, loss = 0.01647169\n",
      "Iteration 64, loss = 0.01615259\n",
      "Iteration 65, loss = 0.01713606\n",
      "Iteration 66, loss = 0.01637231\n",
      "Iteration 67, loss = 0.01525790\n",
      "Iteration 68, loss = 0.01676453\n",
      "Iteration 69, loss = 0.01496914\n",
      "Iteration 70, loss = 0.01619795\n",
      "Iteration 71, loss = 0.01609561\n",
      "Iteration 72, loss = 0.01598583\n",
      "Iteration 73, loss = 0.01658426\n",
      "Iteration 74, loss = 0.01547512\n",
      "Iteration 75, loss = 0.01565002\n",
      "Iteration 76, loss = 0.01559873\n",
      "Iteration 77, loss = 0.01496976\n",
      "Iteration 78, loss = 0.01513543\n",
      "Iteration 79, loss = 0.01580034\n",
      "Iteration 80, loss = 0.01542585\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05858183\n",
      "Iteration 2, loss = 0.03555955\n",
      "Iteration 3, loss = 0.02875305\n",
      "Iteration 4, loss = 0.02761533\n",
      "Iteration 5, loss = 0.02535729\n",
      "Iteration 6, loss = 0.02697921\n",
      "Iteration 7, loss = 0.02636014\n",
      "Iteration 8, loss = 0.02557400\n",
      "Iteration 9, loss = 0.02293921\n",
      "Iteration 10, loss = 0.02332848\n",
      "Iteration 11, loss = 0.02146371\n",
      "Iteration 12, loss = 0.02136125\n",
      "Iteration 13, loss = 0.02061713\n",
      "Iteration 14, loss = 0.02089512\n",
      "Iteration 15, loss = 0.01983038\n",
      "Iteration 16, loss = 0.02067704\n",
      "Iteration 17, loss = 0.02125462\n",
      "Iteration 18, loss = 0.01901070\n",
      "Iteration 19, loss = 0.02382949\n",
      "Iteration 20, loss = 0.01804448\n",
      "Iteration 21, loss = 0.01914299\n",
      "Iteration 22, loss = 0.01821930\n",
      "Iteration 23, loss = 0.01912483\n",
      "Iteration 24, loss = 0.01921920\n",
      "Iteration 25, loss = 0.01884053\n",
      "Iteration 26, loss = 0.01849127\n",
      "Iteration 27, loss = 0.01787240\n",
      "Iteration 28, loss = 0.01923987\n",
      "Iteration 29, loss = 0.01836800\n",
      "Iteration 30, loss = 0.01741439\n",
      "Iteration 31, loss = 0.01811956\n",
      "Iteration 32, loss = 0.01908929\n",
      "Iteration 33, loss = 0.01691609\n",
      "Iteration 34, loss = 0.01742262\n",
      "Iteration 35, loss = 0.01704767\n",
      "Iteration 36, loss = 0.01703810\n",
      "Iteration 37, loss = 0.01867056\n",
      "Iteration 38, loss = 0.01782380\n",
      "Iteration 39, loss = 0.01765004\n",
      "Iteration 40, loss = 0.01846645\n",
      "Iteration 41, loss = 0.01723215\n",
      "Iteration 42, loss = 0.01758284\n",
      "Iteration 43, loss = 0.01753229\n",
      "Iteration 44, loss = 0.01745431\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06070109\n",
      "Iteration 2, loss = 0.03500549\n",
      "Iteration 3, loss = 0.03175582\n",
      "Iteration 4, loss = 0.02675331\n",
      "Iteration 5, loss = 0.02782841\n",
      "Iteration 6, loss = 0.02487273\n",
      "Iteration 7, loss = 0.02437394\n",
      "Iteration 8, loss = 0.02378572\n",
      "Iteration 9, loss = 0.02227462\n",
      "Iteration 10, loss = 0.02167120\n",
      "Iteration 11, loss = 0.02151491\n",
      "Iteration 12, loss = 0.02200226\n",
      "Iteration 13, loss = 0.02207795\n",
      "Iteration 14, loss = 0.02301839\n",
      "Iteration 15, loss = 0.02020490\n",
      "Iteration 16, loss = 0.02003513\n",
      "Iteration 17, loss = 0.02070573\n",
      "Iteration 18, loss = 0.01982221\n",
      "Iteration 19, loss = 0.01956762\n",
      "Iteration 20, loss = 0.01901274\n",
      "Iteration 21, loss = 0.01951167\n",
      "Iteration 22, loss = 0.01927648\n",
      "Iteration 23, loss = 0.02070483\n",
      "Iteration 24, loss = 0.01906716\n",
      "Iteration 25, loss = 0.01870423\n",
      "Iteration 26, loss = 0.01788264\n",
      "Iteration 27, loss = 0.01863550\n",
      "Iteration 28, loss = 0.01759121\n",
      "Iteration 29, loss = 0.01819979\n",
      "Iteration 30, loss = 0.01965493\n",
      "Iteration 31, loss = 0.01776425\n",
      "Iteration 32, loss = 0.01756363\n",
      "Iteration 33, loss = 0.01818471\n",
      "Iteration 34, loss = 0.01794204\n",
      "Iteration 35, loss = 0.01794054\n",
      "Iteration 36, loss = 0.01775680\n",
      "Iteration 37, loss = 0.01712472\n",
      "Iteration 38, loss = 0.01923352\n",
      "Iteration 39, loss = 0.01695143\n",
      "Iteration 40, loss = 0.01687583\n",
      "Iteration 41, loss = 0.01737761\n",
      "Iteration 42, loss = 0.01712601\n",
      "Iteration 43, loss = 0.01679252\n",
      "Iteration 44, loss = 0.01671726\n",
      "Iteration 45, loss = 0.01741111\n",
      "Iteration 46, loss = 0.01676504\n",
      "Iteration 47, loss = 0.01662684\n",
      "Iteration 48, loss = 0.01636622\n",
      "Iteration 49, loss = 0.01590846\n",
      "Iteration 50, loss = 0.01641386\n",
      "Iteration 51, loss = 0.01611406\n",
      "Iteration 52, loss = 0.01588754\n",
      "Iteration 53, loss = 0.01577039\n",
      "Iteration 54, loss = 0.01730771\n",
      "Iteration 55, loss = 0.01557952\n",
      "Iteration 56, loss = 0.01595892\n",
      "Iteration 57, loss = 0.01627865\n",
      "Iteration 58, loss = 0.01608911\n",
      "Iteration 59, loss = 0.01664935\n",
      "Iteration 60, loss = 0.01547624\n",
      "Iteration 61, loss = 0.01687339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 62, loss = 0.01599423\n",
      "Iteration 63, loss = 0.01514558\n",
      "Iteration 64, loss = 0.01736880\n",
      "Iteration 65, loss = 0.01550232\n",
      "Iteration 66, loss = 0.01613183\n",
      "Iteration 67, loss = 0.01527053\n",
      "Iteration 68, loss = 0.01548325\n",
      "Iteration 69, loss = 0.01578373\n",
      "Iteration 70, loss = 0.01661978\n",
      "Iteration 71, loss = 0.01562612\n",
      "Iteration 72, loss = 0.01578155\n",
      "Iteration 73, loss = 0.01476794\n",
      "Iteration 74, loss = 0.01525808\n",
      "Iteration 75, loss = 0.01638250\n",
      "Iteration 76, loss = 0.01561332\n",
      "Iteration 77, loss = 0.01511508\n",
      "Iteration 78, loss = 0.01557894\n",
      "Iteration 79, loss = 0.01523463\n",
      "Iteration 80, loss = 0.01477837\n",
      "Iteration 81, loss = 0.01499128\n",
      "Iteration 82, loss = 0.01517547\n",
      "Iteration 83, loss = 0.01536337\n",
      "Iteration 84, loss = 0.01600868\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05984926\n",
      "Iteration 2, loss = 0.03823009\n",
      "Iteration 3, loss = 0.03115289\n",
      "Iteration 4, loss = 0.02867560\n",
      "Iteration 5, loss = 0.02602245\n",
      "Iteration 6, loss = 0.02543165\n",
      "Iteration 7, loss = 0.02537695\n",
      "Iteration 8, loss = 0.02329891\n",
      "Iteration 9, loss = 0.02256517\n",
      "Iteration 10, loss = 0.02109828\n",
      "Iteration 11, loss = 0.02272006\n",
      "Iteration 12, loss = 0.02685397\n",
      "Iteration 13, loss = 0.02539214\n",
      "Iteration 14, loss = 0.02451628\n",
      "Iteration 15, loss = 0.02394433\n",
      "Iteration 16, loss = 0.02223913\n",
      "Iteration 17, loss = 0.02116626\n",
      "Iteration 18, loss = 0.02174278\n",
      "Iteration 19, loss = 0.02141063\n",
      "Iteration 20, loss = 0.02146688\n",
      "Iteration 21, loss = 0.02078131\n",
      "Iteration 22, loss = 0.02147011\n",
      "Iteration 23, loss = 0.02135432\n",
      "Iteration 24, loss = 0.02093748\n",
      "Iteration 25, loss = 0.01978805\n",
      "Iteration 26, loss = 0.01941656\n",
      "Iteration 27, loss = 0.01955413\n",
      "Iteration 28, loss = 0.01937646\n",
      "Iteration 29, loss = 0.01871164\n",
      "Iteration 30, loss = 0.01912386\n",
      "Iteration 31, loss = 0.02059703\n",
      "Iteration 32, loss = 0.01850523\n",
      "Iteration 33, loss = 0.01847383\n",
      "Iteration 34, loss = 0.01821045\n",
      "Iteration 35, loss = 0.01786392\n",
      "Iteration 36, loss = 0.01961919\n",
      "Iteration 37, loss = 0.01825370\n",
      "Iteration 38, loss = 0.01860952\n",
      "Iteration 39, loss = 0.01945073\n",
      "Iteration 40, loss = 0.01805716\n",
      "Iteration 41, loss = 0.01807121\n",
      "Iteration 42, loss = 0.01801864\n",
      "Iteration 43, loss = 0.01987517\n",
      "Iteration 44, loss = 0.01742646\n",
      "Iteration 45, loss = 0.01759730\n",
      "Iteration 46, loss = 0.01780775\n",
      "Iteration 47, loss = 0.01864078\n",
      "Iteration 48, loss = 0.01775746\n",
      "Iteration 49, loss = 0.01669479\n",
      "Iteration 50, loss = 0.01831882\n",
      "Iteration 51, loss = 0.01809204\n",
      "Iteration 52, loss = 0.01762862\n",
      "Iteration 53, loss = 0.01932075\n",
      "Iteration 54, loss = 0.01726852\n",
      "Iteration 55, loss = 0.01739786\n",
      "Iteration 56, loss = 0.01671943\n",
      "Iteration 57, loss = 0.01817417\n",
      "Iteration 58, loss = 0.01745107\n",
      "Iteration 59, loss = 0.01772941\n",
      "Iteration 60, loss = 0.01717824\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05916115\n",
      "Iteration 2, loss = 0.03532745\n",
      "Iteration 3, loss = 0.02865720\n",
      "Iteration 4, loss = 0.02913167\n",
      "Iteration 5, loss = 0.02771673\n",
      "Iteration 6, loss = 0.02511624\n",
      "Iteration 7, loss = 0.02477112\n",
      "Iteration 8, loss = 0.02321385\n",
      "Iteration 9, loss = 0.02408300\n",
      "Iteration 10, loss = 0.02060835\n",
      "Iteration 11, loss = 0.02242799\n",
      "Iteration 12, loss = 0.02088649\n",
      "Iteration 13, loss = 0.01978157\n",
      "Iteration 14, loss = 0.02254943\n",
      "Iteration 15, loss = 0.02167020\n",
      "Iteration 16, loss = 0.01889911\n",
      "Iteration 17, loss = 0.02246090\n",
      "Iteration 18, loss = 0.02025388\n",
      "Iteration 19, loss = 0.01988468\n",
      "Iteration 20, loss = 0.02019532\n",
      "Iteration 21, loss = 0.02111671\n",
      "Iteration 22, loss = 0.01941290\n",
      "Iteration 23, loss = 0.01907753\n",
      "Iteration 24, loss = 0.02250697\n",
      "Iteration 25, loss = 0.01849733\n",
      "Iteration 26, loss = 0.01872984\n",
      "Iteration 27, loss = 0.01812764\n",
      "Iteration 28, loss = 0.01796058\n",
      "Iteration 29, loss = 0.01858429\n",
      "Iteration 30, loss = 0.01881039\n",
      "Iteration 31, loss = 0.01736864\n",
      "Iteration 32, loss = 0.01790517\n",
      "Iteration 33, loss = 0.01748032\n",
      "Iteration 34, loss = 0.01724366\n",
      "Iteration 35, loss = 0.01776634\n",
      "Iteration 36, loss = 0.01706103\n",
      "Iteration 37, loss = 0.01793623\n",
      "Iteration 38, loss = 0.01716498\n",
      "Iteration 39, loss = 0.01776779\n",
      "Iteration 40, loss = 0.01819445\n",
      "Iteration 41, loss = 0.01749922\n",
      "Iteration 42, loss = 0.01706182\n",
      "Iteration 43, loss = 0.01703359\n",
      "Iteration 44, loss = 0.01641201\n",
      "Iteration 45, loss = 0.01712532\n",
      "Iteration 46, loss = 0.01688597\n",
      "Iteration 47, loss = 0.01715263\n",
      "Iteration 48, loss = 0.01752385\n",
      "Iteration 49, loss = 0.01648874\n",
      "Iteration 50, loss = 0.01611594\n",
      "Iteration 51, loss = 0.01599395\n",
      "Iteration 52, loss = 0.01608201\n",
      "Iteration 53, loss = 0.01633748\n",
      "Iteration 54, loss = 0.01631454\n",
      "Iteration 55, loss = 0.01603688\n",
      "Iteration 56, loss = 0.01722735\n",
      "Iteration 57, loss = 0.01674481\n",
      "Iteration 58, loss = 0.01688104\n",
      "Iteration 59, loss = 0.01535422\n",
      "Iteration 60, loss = 0.01575431\n",
      "Iteration 61, loss = 0.01533757\n",
      "Iteration 62, loss = 0.01675365\n",
      "Iteration 63, loss = 0.01651083\n",
      "Iteration 64, loss = 0.01643124\n",
      "Iteration 65, loss = 0.01575061\n",
      "Iteration 66, loss = 0.01555953\n",
      "Iteration 67, loss = 0.01621673\n",
      "Iteration 68, loss = 0.01584978\n",
      "Iteration 69, loss = 0.01591931\n",
      "Iteration 70, loss = 0.01607139\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05874528\n",
      "Iteration 2, loss = 0.03448290\n",
      "Iteration 3, loss = 0.02716455\n",
      "Iteration 4, loss = 0.02819176\n",
      "Iteration 5, loss = 0.02643716\n",
      "Iteration 6, loss = 0.02538788\n",
      "Iteration 7, loss = 0.02427645\n",
      "Iteration 8, loss = 0.02401556\n",
      "Iteration 9, loss = 0.02238394\n",
      "Iteration 10, loss = 0.02175867\n",
      "Iteration 11, loss = 0.02256496\n",
      "Iteration 12, loss = 0.02129522\n",
      "Iteration 13, loss = 0.02166344\n",
      "Iteration 14, loss = 0.02002928\n",
      "Iteration 15, loss = 0.01951243\n",
      "Iteration 16, loss = 0.02161059\n",
      "Iteration 17, loss = 0.01966475\n",
      "Iteration 18, loss = 0.01830738\n",
      "Iteration 19, loss = 0.01896330\n",
      "Iteration 20, loss = 0.01864641\n",
      "Iteration 21, loss = 0.01912091\n",
      "Iteration 22, loss = 0.01844636\n",
      "Iteration 23, loss = 0.01885517\n",
      "Iteration 24, loss = 0.01919136\n",
      "Iteration 25, loss = 0.01966969\n",
      "Iteration 26, loss = 0.01797841\n",
      "Iteration 27, loss = 0.01865419\n",
      "Iteration 28, loss = 0.01868844\n",
      "Iteration 29, loss = 0.01769135\n",
      "Iteration 30, loss = 0.01824426\n",
      "Iteration 31, loss = 0.01833390\n",
      "Iteration 32, loss = 0.01815108\n",
      "Iteration 33, loss = 0.01819616\n",
      "Iteration 34, loss = 0.01751834\n",
      "Iteration 35, loss = 0.01792154\n",
      "Iteration 36, loss = 0.01819685\n",
      "Iteration 37, loss = 0.01735866\n",
      "Iteration 38, loss = 0.01882927\n",
      "Iteration 39, loss = 0.01661274\n",
      "Iteration 40, loss = 0.01711354\n",
      "Iteration 41, loss = 0.01835992\n",
      "Iteration 42, loss = 0.01662846\n",
      "Iteration 43, loss = 0.01837581\n",
      "Iteration 44, loss = 0.01794541\n",
      "Iteration 45, loss = 0.01750066\n",
      "Iteration 46, loss = 0.01683205\n",
      "Iteration 47, loss = 0.01675556\n",
      "Iteration 48, loss = 0.01663496\n",
      "Iteration 49, loss = 0.01686922\n",
      "Iteration 50, loss = 0.01598600\n",
      "Iteration 51, loss = 0.01733373\n",
      "Iteration 52, loss = 0.01668239\n",
      "Iteration 53, loss = 0.01698099\n",
      "Iteration 54, loss = 0.01621093\n",
      "Iteration 55, loss = 0.01587127\n",
      "Iteration 56, loss = 0.01645933\n",
      "Iteration 57, loss = 0.01594885\n",
      "Iteration 58, loss = 0.01636873\n",
      "Iteration 59, loss = 0.01585727\n",
      "Iteration 60, loss = 0.01589292\n",
      "Iteration 61, loss = 0.01688415\n",
      "Iteration 62, loss = 0.01580869\n",
      "Iteration 63, loss = 0.01544687\n",
      "Iteration 64, loss = 0.01725404\n",
      "Iteration 65, loss = 0.01558268\n",
      "Iteration 66, loss = 0.01611041\n",
      "Iteration 67, loss = 0.01560217\n",
      "Iteration 68, loss = 0.01562047\n",
      "Iteration 69, loss = 0.01602605\n",
      "Iteration 70, loss = 0.01621542\n",
      "Iteration 71, loss = 0.01583949\n",
      "Iteration 72, loss = 0.01541185\n",
      "Iteration 73, loss = 0.01612326\n",
      "Iteration 74, loss = 0.01578371\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06328349\n",
      "Iteration 2, loss = 0.04108646\n",
      "Iteration 3, loss = 0.03571487\n",
      "Iteration 4, loss = 0.03250130\n",
      "Iteration 5, loss = 0.02864861\n",
      "Iteration 6, loss = 0.02985214\n",
      "Iteration 7, loss = 0.02876468\n",
      "Iteration 8, loss = 0.02726437\n",
      "Iteration 9, loss = 0.02675217\n",
      "Iteration 10, loss = 0.02647792\n",
      "Iteration 11, loss = 0.02571874\n",
      "Iteration 12, loss = 0.02565132\n",
      "Iteration 13, loss = 0.02576313\n",
      "Iteration 14, loss = 0.02492520\n",
      "Iteration 15, loss = 0.02730091\n",
      "Iteration 16, loss = 0.02421254\n",
      "Iteration 17, loss = 0.02403322\n",
      "Iteration 18, loss = 0.02362413\n",
      "Iteration 19, loss = 0.02282675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20, loss = 0.02147838\n",
      "Iteration 21, loss = 0.02359376\n",
      "Iteration 22, loss = 0.02364367\n",
      "Iteration 23, loss = 0.02292047\n",
      "Iteration 24, loss = 0.02335331\n",
      "Iteration 25, loss = 0.02177221\n",
      "Iteration 26, loss = 0.02101720\n",
      "Iteration 27, loss = 0.01951232\n",
      "Iteration 28, loss = 0.02136317\n",
      "Iteration 29, loss = 0.02159636\n",
      "Iteration 30, loss = 0.02192984\n",
      "Iteration 31, loss = 0.02176727\n",
      "Iteration 32, loss = 0.02113155\n",
      "Iteration 33, loss = 0.02171158\n",
      "Iteration 34, loss = 0.01911291\n",
      "Iteration 35, loss = 0.01783596\n",
      "Iteration 36, loss = 0.01743676\n",
      "Iteration 37, loss = 0.01797515\n",
      "Iteration 38, loss = 0.02025816\n",
      "Iteration 39, loss = 0.01815964\n",
      "Iteration 40, loss = 0.01617043\n",
      "Iteration 41, loss = 0.01749614\n",
      "Iteration 42, loss = 0.01786424\n",
      "Iteration 43, loss = 0.01728262\n",
      "Iteration 44, loss = 0.02029719\n",
      "Iteration 45, loss = 0.02011033\n",
      "Iteration 46, loss = 0.02088515\n",
      "Iteration 47, loss = 0.02162862\n",
      "Iteration 48, loss = 0.02111137\n",
      "Iteration 49, loss = 0.02060501\n",
      "Iteration 50, loss = 0.01976423\n",
      "Iteration 51, loss = 0.02289755\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.06218624\n",
      "Iteration 2, loss = 0.03668071\n",
      "Iteration 3, loss = 0.03226282\n",
      "Iteration 4, loss = 0.02819382\n",
      "Iteration 5, loss = 0.02685102\n",
      "Iteration 6, loss = 0.02958742\n",
      "Iteration 7, loss = 0.02518030\n",
      "Iteration 8, loss = 0.02395753\n",
      "Iteration 9, loss = 0.02309846\n",
      "Iteration 10, loss = 0.02153443\n",
      "Iteration 11, loss = 0.02089064\n",
      "Iteration 12, loss = 0.02763736\n",
      "Iteration 13, loss = 0.03197841\n",
      "Iteration 14, loss = 0.02950784\n",
      "Iteration 15, loss = 0.02702780\n",
      "Iteration 16, loss = 0.02878749\n",
      "Iteration 17, loss = 0.02607206\n",
      "Iteration 18, loss = 0.02574858\n",
      "Iteration 19, loss = 0.02505762\n",
      "Iteration 20, loss = 0.02471078\n",
      "Iteration 21, loss = 0.02333101\n",
      "Iteration 22, loss = 0.02256000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.05947497\n",
      "Iteration 2, loss = 0.03764550\n",
      "Iteration 3, loss = 0.02890009\n",
      "Iteration 4, loss = 0.02780977\n",
      "Iteration 5, loss = 0.02893640\n",
      "Iteration 6, loss = 0.02611998\n",
      "Iteration 7, loss = 0.02421509\n",
      "Iteration 8, loss = 0.02716672\n",
      "Iteration 9, loss = 0.02150714\n",
      "Iteration 10, loss = 0.02228034\n",
      "Iteration 11, loss = 0.02198460\n",
      "Iteration 12, loss = 0.02090187\n",
      "Iteration 13, loss = 0.01988785\n",
      "Iteration 14, loss = 0.02250026\n",
      "Iteration 15, loss = 0.02095438\n",
      "Iteration 16, loss = 0.02033488\n",
      "Iteration 17, loss = 0.01974992\n",
      "Iteration 18, loss = 0.02102240\n",
      "Iteration 19, loss = 0.01875900\n",
      "Iteration 20, loss = 0.02081969\n",
      "Iteration 21, loss = 0.01796866\n",
      "Iteration 22, loss = 0.01877721\n",
      "Iteration 23, loss = 0.01957089\n",
      "Iteration 24, loss = 0.01900169\n",
      "Iteration 25, loss = 0.01780629\n",
      "Iteration 26, loss = 0.01800911\n",
      "Iteration 27, loss = 0.01842773\n",
      "Iteration 28, loss = 0.01724758\n",
      "Iteration 29, loss = 0.01820799\n",
      "Iteration 30, loss = 0.01950083\n",
      "Iteration 31, loss = 0.01697103\n",
      "Iteration 32, loss = 0.01681281\n",
      "Iteration 33, loss = 0.01826582\n",
      "Iteration 34, loss = 0.01733864\n",
      "Iteration 35, loss = 0.01639940\n",
      "Iteration 36, loss = 0.01736540\n",
      "Iteration 37, loss = 0.01642467\n",
      "Iteration 38, loss = 0.01751202\n",
      "Iteration 39, loss = 0.01795626\n",
      "Iteration 40, loss = 0.01674122\n",
      "Iteration 41, loss = 0.01645725\n",
      "Iteration 42, loss = 0.01789955\n",
      "Iteration 43, loss = 0.01679882\n",
      "Iteration 44, loss = 0.01700815\n",
      "Iteration 45, loss = 0.01676711\n",
      "Iteration 46, loss = 0.01777105\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Mean accuracy = 0.9930175722561643\n",
      "Mean sensitivity = 0.9920264596112525\n",
      "Mean specificity = 0.9945110279893552\n",
      "Mean precision = 0.9957640069116668\n",
      "Mean recall = 0.9920264596112525\n",
      "Mean f1-score = 0.9938594829733234\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "data = pd.read_csv('ds3_normalized.csv')\n",
    "X = data[['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'AA', 'AB', 'AC', 'AD', 'AE', 'AF', 'AG', 'AH', 'AI', 'AJ', 'AK', 'AL', 'AM', 'AN', 'AO', 'AP', 'AQ', 'AR', 'AS', 'AT', 'AU', 'AV', 'AW', 'AX', 'AY', 'AZ', 'BA', 'BB', 'BC', 'BD', 'BE', 'BF', 'BG', 'BH', 'BI', 'BJ', 'BK', 'BL', 'BM', 'BN', 'BO', 'BP', 'BQ', 'BR']]\n",
    "y = data['BS']\n",
    "\n",
    "# Convert target variable to a one-dimensional array\n",
    "y = y.ravel()\n",
    "\n",
    "# Initialize the MLP Classifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(20, 10), random_state=5, verbose=True, learning_rate_init=0.01)\n",
    "\n",
    "# Define the number of folds (k)\n",
    "num_folds = 10\n",
    "\n",
    "# Initialize KFold with the specified number of folds\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store evaluation metrics\n",
    "accuracies = []\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Train the model\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Evaluate the model and calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    tp = np.sum((y_pred == 1) & (y_test == 1))\n",
    "    tn = np.sum((y_pred == -1) & (y_test == -1))\n",
    "    fp = np.sum((y_pred == -1) & (y_test == 1))\n",
    "    fn = np.sum((y_pred == 1) & (y_test == -1))\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "    sensitivities.append(sensitivity)\n",
    "    specificities.append(specificity)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1_score)\n",
    "\n",
    "# Calculate the average metrics across all folds\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "mean_sensitivity = np.mean(sensitivities)\n",
    "mean_specificity = np.mean(specificities)\n",
    "mean_precision = np.mean(precisions)\n",
    "mean_recall = np.mean(recalls)\n",
    "mean_f1_score = np.mean(f1_scores)\n",
    "\n",
    "print('Mean accuracy =', mean_accuracy)\n",
    "print('Mean sensitivity =', mean_sensitivity)\n",
    "print('Mean specificity =', mean_specificity)\n",
    "print('Mean precision =', mean_precision)\n",
    "print('Mean recall =', mean_recall)\n",
    "print('Mean f1-score =', mean_f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8828f702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy = 0.9999113903300112\n",
      "Mean sensitivity = 0.9999531699772826\n",
      "Mean specificity = 0.9998568812983601\n",
      "Mean precision = 0.9998905338079297\n",
      "Mean recall = 0.9999531699772826\n",
      "Mean f1-score = 0.9999218494370922\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "data = pd.read_csv('ds3_normalized.csv')\n",
    "X = data[['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'AA', 'AB', 'AC', 'AD', 'AE', 'AF', 'AG', 'AH', 'AI', 'AJ', 'AK', 'AL', 'AM', 'AN', 'AO', 'AP', 'AQ', 'AR', 'AS', 'AT', 'AU', 'AV', 'AW', 'AX', 'AY', 'AZ', 'BA', 'BB', 'BC', 'BD', 'BE', 'BF', 'BG', 'BH', 'BI', 'BJ', 'BK', 'BL', 'BM', 'BN', 'BO', 'BP', 'BQ', 'BR']]\n",
    "y = data['BS']\n",
    "\n",
    "# Convert target variable to a one-dimensional array\n",
    "y = y.ravel()\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "forest = RandomForestClassifier(criterion='gini', n_estimators=5, random_state=1, n_jobs=2)\n",
    "\n",
    "# Define the number of folds (k)\n",
    "num_folds = 10\n",
    "\n",
    "# Initialize KFold with the specified number of folds\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store evaluation metrics\n",
    "accuracies = []\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Train the model\n",
    "    forest.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = forest.predict(X_test)\n",
    "\n",
    "    # Evaluate the model and calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    tp = np.sum((y_pred == 1) & (y_test == 1))\n",
    "    tn = np.sum((y_pred == -1) & (y_test == -1))\n",
    "    fp = np.sum((y_pred == -1) & (y_test == 1))\n",
    "    fn = np.sum((y_pred == 1) & (y_test == -1))\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "    sensitivities.append(sensitivity)\n",
    "    specificities.append(specificity)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1_score)\n",
    "\n",
    "# Calculate the average metrics across all folds\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "mean_sensitivity = np.mean(sensitivities)\n",
    "mean_specificity = np.mean(specificities)\n",
    "mean_precision = np.mean(precisions)\n",
    "mean_recall = np.mean(recalls)\n",
    "mean_f1_score = np.mean(f1_scores)\n",
    "\n",
    "print('Mean accuracy =', mean_accuracy)\n",
    "print('Mean sensitivity =', mean_sensitivity)\n",
    "print('Mean specificity =', mean_specificity)\n",
    "print('Mean precision =', mean_precision)\n",
    "print('Mean recall =', mean_recall)\n",
    "print('Mean f1-score =', mean_f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c2520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "data=pd.read_csv('ds3_normalized.csv')\n",
    "X=data[['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z','AA','AB','AC','AD','AE','AF','AG','AH','AI','AJ','AK','AL','AM','AN','AO','AP','AQ','AR','AS','AT','AU','AV','AW','AX','AY','AZ','BA','BB','BC','BD','BE','BF','BG','BH','BI','BJ','BK','BL','BM','BN','BO','BP','BQ','BR']]\n",
    "y=data['BS']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ffa628e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>J</th>\n",
       "      <th>...</th>\n",
       "      <th>BJ</th>\n",
       "      <th>BK</th>\n",
       "      <th>BL</th>\n",
       "      <th>BM</th>\n",
       "      <th>BN</th>\n",
       "      <th>BO</th>\n",
       "      <th>BP</th>\n",
       "      <th>BQ</th>\n",
       "      <th>BR</th>\n",
       "      <th>BS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.96315</td>\n",
       "      <td>-0.78457</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.88299</td>\n",
       "      <td>-0.61528</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.98798</td>\n",
       "      <td>-0.89144</td>\n",
       "      <td>-0.96915</td>\n",
       "      <td>-0.86827</td>\n",
       "      <td>-0.91658</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.87268</td>\n",
       "      <td>-0.89746</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.92415</td>\n",
       "      <td>-0.82435</td>\n",
       "      <td>-0.780</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>-0.84727</td>\n",
       "      <td>-0.87931</td>\n",
       "      <td>-0.71333</td>\n",
       "      <td>-0.91530</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.99997</td>\n",
       "      <td>-0.79803</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.82964</td>\n",
       "      <td>-0.45823</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>-0.85864</td>\n",
       "      <td>-0.85558</td>\n",
       "      <td>-0.70000</td>\n",
       "      <td>-0.90841</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.90047</td>\n",
       "      <td>-0.85293</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.91570</td>\n",
       "      <td>-0.60502</td>\n",
       "      <td>-0.572</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>-0.85323</td>\n",
       "      <td>-0.89725</td>\n",
       "      <td>-0.74833</td>\n",
       "      <td>-0.91120</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.95589</td>\n",
       "      <td>-0.82393</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.82776</td>\n",
       "      <td>-0.66350</td>\n",
       "      <td>-0.612</td>\n",
       "      <td>-0.98987</td>\n",
       "      <td>-0.84832</td>\n",
       "      <td>-0.88457</td>\n",
       "      <td>-0.74667</td>\n",
       "      <td>-0.91184</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B    C        D        E    F    G    H    I    J  ...   BJ  \\\n",
       "0 -1.0 -1.0 -1.0  0.96315 -0.78457 -1.0 -1.0 -1.0 -1.0 -1.0  ... -1.0   \n",
       "1 -1.0 -1.0 -1.0  0.87268 -0.89746 -1.0 -1.0 -1.0 -1.0 -1.0  ... -1.0   \n",
       "2 -1.0 -1.0 -1.0  0.99997 -0.79803 -1.0 -1.0 -1.0 -1.0 -1.0  ... -1.0   \n",
       "3 -1.0 -1.0 -1.0  0.90047 -0.85293 -1.0 -1.0 -1.0 -1.0 -1.0  ... -1.0   \n",
       "4 -1.0 -1.0 -1.0  0.95589 -0.82393 -1.0 -1.0 -1.0 -1.0 -1.0  ... -1.0   \n",
       "\n",
       "        BK       BL     BM       BN       BO       BP       BQ       BR  BS  \n",
       "0 -0.88299 -0.61528 -0.500 -0.98798 -0.89144 -0.96915 -0.86827 -0.91658  -1  \n",
       "1 -0.92415 -0.82435 -0.780 -1.00000 -0.84727 -0.87931 -0.71333 -0.91530  -1  \n",
       "2 -0.82964 -0.45823 -0.520 -1.00000 -0.85864 -0.85558 -0.70000 -0.90841  -1  \n",
       "3 -0.91570 -0.60502 -0.572 -1.00000 -0.85323 -0.89725 -0.74833 -0.91120  -1  \n",
       "4 -0.82776 -0.66350 -0.612 -0.98987 -0.84832 -0.88457 -0.74667 -0.91184  -1  \n",
       "\n",
       "[5 rows x 71 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02746ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time of train =  1.5369133949279785  sec\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------- Random Forest Classification, Train\n",
    "start = time.time()\n",
    "forest = RandomForestClassifier(criterion='gini',\n",
    "                                 n_estimators=5,\n",
    "                                 random_state=1,\n",
    "                                 n_jobs=2)\n",
    "\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "end = time.time()\n",
    "print('time of train = ' , end - start, ' sec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f0a14a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time of test =  0.03931474685668945  sec\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------- Random Forest Classification, Test\n",
    "start = time.time()\n",
    "y_pred = forest.predict(X_test)\n",
    "end = time.time()\n",
    "print('time of test = ' , end - start, ' sec')\n",
    "\n",
    "#--------------------------------------------------------- Random Forest Classification, Evaluations\n",
    "#print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "y_test=y_test.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b4b5ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP =  12813\n",
      "TN =  9753\n",
      "FP =  4\n",
      "FN =  1\n",
      "accuracy =  0.9997784768065217\n",
      "sensitivity =  0.9999219603558608\n",
      "specificity =  0.9995900379214923\n",
      "precision =  0.9996879144885699\n",
      "recall =  0.9999219603558608\n",
      "f1-score =  0.9998049237251765\n"
     ]
    }
   ],
   "source": [
    "#ypred=ypred.ravel()\n",
    "\n",
    "TP=0\n",
    "TN=0\n",
    "FP=0\n",
    "FN=0\n",
    "\n",
    "for i in range(0,len(y_test),1):\n",
    "    if y_pred[i] == 1 and y_test[i] == 1:   TP=TP+1\n",
    "    if y_pred[i] == -1 and y_test[i] == -1: TN=TN+1\n",
    "    if y_pred[i] == -1 and y_test[i] == 1:  FP=FP+1\n",
    "    if y_pred[i] == 1 and y_test[i] == -1:  FN=FN+1\n",
    "\n",
    "print('TP = ' , TP)\n",
    "print('TN = ' , TN)\n",
    "print('FP = ' , FP)\n",
    "print('FN = ' , FN)\n",
    "\n",
    "accuracy=(TP+TN)/(TP+TN+FP+FN);\n",
    "sensitivity=TP/(TP+FN)\n",
    "specificity=TN/(TN+FP)\n",
    "precision=TP/(TP+FP);\n",
    "recall=TP/(TP+FN);\n",
    "f1=2*(precision*recall)/(precision+recall);\n",
    "\n",
    "print('accuracy = ' , accuracy)\n",
    "print('sensitivity = ' , sensitivity)\n",
    "print('specificity = ' , specificity)\n",
    "print('precision = ' , precision)\n",
    "print('recall = ' , recall)\n",
    "print('f1-score = ' , f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc8bd92",
   "metadata": {},
   "source": [
    "2. MLP classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "353485c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.06266024\n",
      "Iteration 2, loss = 0.03729090\n",
      "Iteration 3, loss = 0.03518242\n",
      "Iteration 4, loss = 0.03210841\n",
      "Iteration 5, loss = 0.03133717\n",
      "Iteration 6, loss = 0.03036251\n",
      "Iteration 7, loss = 0.02805827\n",
      "Iteration 8, loss = 0.02585358\n",
      "Iteration 9, loss = 0.02408878\n",
      "Iteration 10, loss = 0.02449205\n",
      "Iteration 11, loss = 0.02253264\n",
      "Iteration 12, loss = 0.02202183\n",
      "Iteration 13, loss = 0.02257307\n",
      "Iteration 14, loss = 0.02145698\n",
      "Iteration 15, loss = 0.02116277\n",
      "Iteration 16, loss = 0.02109177\n",
      "Iteration 17, loss = 0.02032637\n",
      "Iteration 18, loss = 0.02066371\n",
      "Iteration 19, loss = 0.02156367\n",
      "Iteration 20, loss = 0.01993838\n",
      "Iteration 21, loss = 0.01907146\n",
      "Iteration 22, loss = 0.01908540\n",
      "Iteration 23, loss = 0.01913366\n",
      "Iteration 24, loss = 0.01864884\n",
      "Iteration 25, loss = 0.01773710\n",
      "Iteration 26, loss = 0.01897369\n",
      "Iteration 27, loss = 0.01884766\n",
      "Iteration 28, loss = 0.01889274\n",
      "Iteration 29, loss = 0.01883625\n",
      "Iteration 30, loss = 0.01895932\n",
      "Iteration 31, loss = 0.01860651\n",
      "Iteration 32, loss = 0.01970130\n",
      "Iteration 33, loss = 0.01940886\n",
      "Iteration 34, loss = 0.01775621\n",
      "Iteration 35, loss = 0.01807248\n",
      "Iteration 36, loss = 0.01789472\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "time of train =  57.409682512283325  sec\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "data=pd.read_csv('ds3_normalized.csv')\n",
    "X=data[['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z','AA','AB','AC','AD','AE','AF','AG','AH','AI','AJ','AK','AL','AM','AN','AO','AP','AQ','AR','AS','AT','AU','AV','AW','AX','AY','AZ','BA','BB','BC','BD','BE','BF','BG','BH','BI','BJ','BK','BL','BM','BN','BO','BP','BQ','BR']]\n",
    "y=data['BS']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)  \n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "start = time.time()\n",
    "clf = MLPClassifier(hidden_layer_sizes=(20,10),\n",
    "                    random_state=5,\n",
    "                    verbose=True,\n",
    "                    learning_rate_init=0.01)\n",
    "# Fit data onto the model\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "end = time.time()\n",
    "print('time of train = ' , end - start, ' sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "426b7761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>J</th>\n",
       "      <th>...</th>\n",
       "      <th>BJ</th>\n",
       "      <th>BK</th>\n",
       "      <th>BL</th>\n",
       "      <th>BM</th>\n",
       "      <th>BN</th>\n",
       "      <th>BO</th>\n",
       "      <th>BP</th>\n",
       "      <th>BQ</th>\n",
       "      <th>BR</th>\n",
       "      <th>BS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.96315</td>\n",
       "      <td>-0.78457</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.88299</td>\n",
       "      <td>-0.61528</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.98798</td>\n",
       "      <td>-0.89144</td>\n",
       "      <td>-0.96915</td>\n",
       "      <td>-0.86827</td>\n",
       "      <td>-0.91658</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.87268</td>\n",
       "      <td>-0.89746</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.92415</td>\n",
       "      <td>-0.82435</td>\n",
       "      <td>-0.780</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>-0.84727</td>\n",
       "      <td>-0.87931</td>\n",
       "      <td>-0.71333</td>\n",
       "      <td>-0.91530</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.99997</td>\n",
       "      <td>-0.79803</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.82964</td>\n",
       "      <td>-0.45823</td>\n",
       "      <td>-0.520</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>-0.85864</td>\n",
       "      <td>-0.85558</td>\n",
       "      <td>-0.70000</td>\n",
       "      <td>-0.90841</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.90047</td>\n",
       "      <td>-0.85293</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.91570</td>\n",
       "      <td>-0.60502</td>\n",
       "      <td>-0.572</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>-0.85323</td>\n",
       "      <td>-0.89725</td>\n",
       "      <td>-0.74833</td>\n",
       "      <td>-0.91120</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.95589</td>\n",
       "      <td>-0.82393</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.82776</td>\n",
       "      <td>-0.66350</td>\n",
       "      <td>-0.612</td>\n",
       "      <td>-0.98987</td>\n",
       "      <td>-0.84832</td>\n",
       "      <td>-0.88457</td>\n",
       "      <td>-0.74667</td>\n",
       "      <td>-0.91184</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B    C        D        E    F    G    H    I    J  ...   BJ  \\\n",
       "0 -1.0 -1.0 -1.0  0.96315 -0.78457 -1.0 -1.0 -1.0 -1.0 -1.0  ... -1.0   \n",
       "1 -1.0 -1.0 -1.0  0.87268 -0.89746 -1.0 -1.0 -1.0 -1.0 -1.0  ... -1.0   \n",
       "2 -1.0 -1.0 -1.0  0.99997 -0.79803 -1.0 -1.0 -1.0 -1.0 -1.0  ... -1.0   \n",
       "3 -1.0 -1.0 -1.0  0.90047 -0.85293 -1.0 -1.0 -1.0 -1.0 -1.0  ... -1.0   \n",
       "4 -1.0 -1.0 -1.0  0.95589 -0.82393 -1.0 -1.0 -1.0 -1.0 -1.0  ... -1.0   \n",
       "\n",
       "        BK       BL     BM       BN       BO       BP       BQ       BR  BS  \n",
       "0 -0.88299 -0.61528 -0.500 -0.98798 -0.89144 -0.96915 -0.86827 -0.91658  -1  \n",
       "1 -0.92415 -0.82435 -0.780 -1.00000 -0.84727 -0.87931 -0.71333 -0.91530  -1  \n",
       "2 -0.82964 -0.45823 -0.520 -1.00000 -0.85864 -0.85558 -0.70000 -0.90841  -1  \n",
       "3 -0.91570 -0.60502 -0.572 -1.00000 -0.85323 -0.89725 -0.74833 -0.91120  -1  \n",
       "4 -0.82776 -0.66350 -0.612 -0.98987 -0.84832 -0.88457 -0.74667 -0.91184  -1  \n",
       "\n",
       "[5 rows x 71 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c20ae34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time of test =  0.25688695907592773  sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "ypred=clf.predict(X_test)\n",
    "\n",
    "end = time.time()\n",
    "print('time of test = ' , end - start, ' sec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8639655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP =  12718\n",
      "TN =  9739\n",
      "FP =  99\n",
      "FN =  15\n"
     ]
    }
   ],
   "source": [
    "y_test=y_test.ravel()\n",
    "\n",
    "TP=0\n",
    "TN=0\n",
    "FP=0\n",
    "FN=0\n",
    "\n",
    "for i in range(0,len(y_test),1):\n",
    "    if ypred[i] == 1 and y_test[i] == 1:   TP=TP+1\n",
    "    if ypred[i] == -1 and y_test[i] == -1: TN=TN+1\n",
    "    if ypred[i] == -1 and y_test[i] == 1:  FP=FP+1\n",
    "    if ypred[i] == 1 and y_test[i] == -1:  FN=FN+1\n",
    "\n",
    "print('TP = ' , TP)\n",
    "print('TN = ' , TN)\n",
    "print('FP = ' , FP)\n",
    "print('FN = ' , FN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c54d7c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.9949492711886935\n",
      "sensitivity =  0.9988219586900181\n",
      "specificity =  0.9899369790607847\n",
      "precision =  0.9922758835921043\n",
      "recall =  0.9988219586900181\n",
      "f1-score =  0.9955381604696674\n"
     ]
    }
   ],
   "source": [
    "accuracy=(TP+TN)/(TP+TN+FP+FN);\n",
    "sensitivity=TP/(TP+FN)\n",
    "specificity=TN/(TN+FP)\n",
    "precision=TP/(TP+FP);\n",
    "recall=TP/(TP+FN);\n",
    "f1=2*(precision*recall)/(precision+recall);\n",
    "\n",
    "print('accuracy = ' , accuracy)\n",
    "print('sensitivity = ' , sensitivity)\n",
    "print('specificity = ' , specificity)\n",
    "print('precision = ' , precision)\n",
    "print('recall = ' , recall)\n",
    "print('f1-score = ' , f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
